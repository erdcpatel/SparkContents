{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UD(A)Fs with PySpark\n",
    "Ref : https://www.inovex.de/blog/efficient-udafs-with-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark by creating a Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"UD(A)F\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def rows_to_pandas(rows):\n",
    "    \"\"\"Converts a Spark Row iterator of a partition to a Pandas DataFrame assuming YARN\n",
    "\n",
    "    Args:\n",
    "        rows: iterator over PySpark Row objects\n",
    "\n",
    "    Returns:\n",
    "        Pandas DataFrame\n",
    "    \"\"\"\n",
    "    first_row, rows = peek(rows)\n",
    "    if not first_row:\n",
    "        _logger.warning(\"Spark DataFrame is empty! Returning empty Pandas DataFrame!\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    first_row_info = [\"{} ({}): {}\".format(k, rtype(first_row[k]), first_row[k])\n",
    "                      for k in first_row.__fields__]\n",
    "    _logger.debug(\"First partition row: {}\".format(first_row_info))\n",
    "    df = pd.DataFrame.from_records(rows, columns=first_row.__fields__)\n",
    "    _logger.debug(\"Converted partition to DataFrame of shape {} with types:\\n{}\".format(df.shape, df.dtypes))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def peek(iterable):\n",
    "    \"\"\"Peek into the first element and return the whole iterator again\n",
    "\n",
    "    Args:\n",
    "        iterable: iterable object like list or iterator\n",
    "\n",
    "    Returns:\n",
    "        tuple of first element and original iterable\n",
    "    \"\"\"\n",
    "    iterable = iter(iterable)\n",
    "    try:\n",
    "        first_elem = next(iterable)\n",
    "    except StopIteration:\n",
    "        return None, iterable\n",
    "    iterable = chain([first_elem], iterable)\n",
    "    return first_elem, iterable\n",
    "\n",
    "\n",
    "def rtype(var):\n",
    "    \"\"\"Heuristic representation for nested types/containers\n",
    "\n",
    "    Args:\n",
    "        var: some (nested) variable\n",
    "\n",
    "    Returns:\n",
    "        str: string representation of nested datatype (NA=Not Available)\n",
    "    \"\"\"\n",
    "    def etype(x):\n",
    "        return type(x).__name__\n",
    "\n",
    "    if isinstance(var, list):\n",
    "        elem_type = etype(var[0]) if var else \"NA\"\n",
    "        return \"List[{}]\".format(elem_type)\n",
    "    elif isinstance(var, dict):\n",
    "        keys = list(var.keys())\n",
    "        if keys:\n",
    "            key = keys[0]\n",
    "            key_type, val_type = etype(key), etype(var[key])\n",
    "        else:\n",
    "            key_type, val_type = \"NA\", \"NA\"\n",
    "        return \"Dict[{}, {}]\".format(key_type, val_type)\n",
    "    elif isinstance(var, tuple):\n",
    "        elem_types = ', '.join(etype(elem) for elem in var)\n",
    "        return \"Tuple[{}]\".format(elem_types)\n",
    "    else:\n",
    "        return etype(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql.types import Row\n",
    "\n",
    "\n",
    "def convert_dtypes(rows):\n",
    "    \"\"\"Converts some Pandas data types to pure Python data types\n",
    "\n",
    "    Args:\n",
    "        rows (array): numpy recarray holding all rows\n",
    "\n",
    "    Returns:\n",
    "        Iterator over lists of row values\n",
    "    \"\"\"\n",
    "    dtype_map = {pd.Timestamp: lambda x: x.to_pydatetime(),\n",
    "                 np.int8: lambda x: int(x),\n",
    "                 np.int16: lambda x: int(x),\n",
    "                 np.int32: lambda x: int(x),\n",
    "                 np.int64: lambda x: int(x),\n",
    "                 np.float16: lambda x: float(x),\n",
    "                 np.float32: lambda x: float(x),\n",
    "                 np.float64: lambda x: float(x),\n",
    "                 np.float128: lambda x: float(x)}\n",
    "    for row in rows:\n",
    "        yield [dtype_map.get(type(elem), lambda x: x)(elem) for elem in row]\n",
    "\n",
    "\n",
    "def pandas_to_rows(df):\n",
    "    \"\"\"Converts Pandas DataFrame to iterator of Row objects\n",
    "\n",
    "    Args:\n",
    "        df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        Iterator over PySpark Row objects\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        _logger.debug(\"Returning nothing\")\n",
    "        return iter([])\n",
    "    if type(df) is pd.Series:\n",
    "        df = df.to_frame().T\n",
    "    if df.empty:\n",
    "        _logger.warning(\"Pandas DataFrame is empty! Returning nothing!\")\n",
    "        return iter([])\n",
    "    _logger.debug(\"Convert DataFrame of shape {} to partition with types:\\n{}\".format(df.shape, df.dtypes))\n",
    "    records = df.to_records(index=False)\n",
    "    records = convert_dtypes(records)\n",
    "    first_row, records = peek(records)\n",
    "    first_row_info = [\"{} ({}): {}\".format(k, rtype(v), v) for k, v in zip(df.columns, first_row)]\n",
    "    _logger.debug(\"First record row: {}\".format(first_row_info))\n",
    "    row = Row(*df.columns)\n",
    "    return (row(*elems) for elems in records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "class pandas_udaf(object):\n",
    "    \"\"\"Decorator for PySpark UDAFs using Pandas\n",
    "\n",
    "    Args:\n",
    "        loglevel (int): minimum loglevel for emitting messages\n",
    "    \"\"\"\n",
    "    def __init__(self, loglevel=logging.INFO):\n",
    "        self.loglevel = loglevel\n",
    "\n",
    "    def __call__(self, func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args):\n",
    "            # use *args to allow decorating methods (incl. self arg)\n",
    "            args = list(args)\n",
    "            setup_logger(loglevel=self.loglevel)\n",
    "            args[-1] = rows_to_pandas(args[-1])\n",
    "            df = func(*args)\n",
    "            return pandas_to_rows(df)\n",
    "        return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def setup_logger(loglevel=logging.INFO, logfile=\"pyspark.log\"):\n",
    "    \"\"\"Setup basic logging for logging on the executor\n",
    "\n",
    "    Args:\n",
    "        loglevel (int): minimum loglevel for emitting messages\n",
    "        logfile (str): name of the logfile\n",
    "    \"\"\"\n",
    "    logformat = \"%(asctime)s %(levelname)s %(module)s.%(funcName)s: %(message)s\"\n",
    "    datefmt = \"%y/%m/%d %H:%M:%S\"\n",
    "    try:\n",
    "        logfile = os.path.join(os.environ['LOG_DIRS'].split(',')[0], logfile)\n",
    "    except (KeyError, IndexError):\n",
    "        logging.basicConfig(level=loglevel,\n",
    "                            stream=sys.stdout, \n",
    "                            format=logformat,\n",
    "                            datefmt=datefmt)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.error(\"LOG_DIRS is not in environment variables or empty, using STDOUT instead.\")\n",
    "\n",
    "    logging.basicConfig(level=loglevel,\n",
    "                        filename=logfile,\n",
    "                        format=logformat,\n",
    "                        datefmt=datefmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark_udaf\n",
    "import logging\n",
    "\n",
    "\n",
    "@pyspark_udaf.pandas_udaf(loglevel=logging.DEBUG)\n",
    "def my_func(df):\n",
    "    if df.empty:\n",
    "        return\n",
    "    df = df.groupby('country').apply(lambda x: x.drop('country', axis=1).describe())\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country level_1  feature1  feature2\n",
      "0      FRA   count  3.000000  3.000000\n",
      "1      FRA    mean  1.000000  5.666667\n",
      "2      FRA     std  1.000000  2.516611\n",
      "3      FRA     min  0.000000  3.000000\n",
      "4      FRA     25%  0.500000  4.500000\n",
      "5      FRA     50%  1.000000  6.000000\n",
      "6      FRA     75%  1.500000  7.000000\n",
      "7      FRA     max  2.000000  8.000000\n",
      "8      DEU   count  3.000000  3.000000\n",
      "9      DEU    mean  2.666667  5.666667\n",
      "10     DEU     std  0.577350  4.041452\n",
      "11     DEU     min  2.000000  1.000000\n",
      "12     DEU     25%  2.500000  4.500000\n",
      "13     DEU     50%  3.000000  8.000000\n",
      "14     DEU     75%  3.000000  8.000000\n",
      "15     DEU     max  3.000000  8.000000\n"
     ]
    }
   ],
   "source": [
    "# make pyspark_udaf.py available to the executors\n",
    "#spark.sparkContext.addFile('./pyspark_udaf.py')\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    data = [('DEU', 2, 1.0), ('DEU', 3, 8.0), ('FRA', 2, 6.0),\n",
    "            ('FRA', 0, 8.0), ('DEU', 3, 8.0), ('FRA', 1, 3.0)],\n",
    "    schema = ['country', 'feature1', 'feature2'])\n",
    "\n",
    "stats_df = df.repartition('country').rdd.mapPartitions(my_func).toDF()\n",
    "print(stats_df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
